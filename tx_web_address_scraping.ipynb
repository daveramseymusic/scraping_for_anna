{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/envs/fastai/lib/python3.8/site-packages (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in /opt/conda/envs/fastai/lib/python3.8/site-packages (from beautifulsoup4) (2.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import re\n",
    "import concurrent.futures\n",
    "import functools\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "## get 20 urls from each search page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets try 1 next\n",
    "#create list of 2-164\n",
    "results_pages_q1 = [ i for i in range(0,10)]\n",
    "results_pages_a = [i for i in range(10,46)]\n",
    "results_pages_b = [i for i in range(46,100)]\n",
    "results_pages_c = [i for i in range(100,165)]\n",
    "results_pages_d = [i for i in range(165,190)]\n",
    "results_pages_e = [i for i in range(190,290)]\n",
    "results_pages_f = [i for i in range(290,292)]\n",
    "results_pages_g = [i for i in range(292,365)]\n",
    "results_pages_h = [i for i in range(365,505)]\n",
    "\n",
    "\n",
    "results_pages_z = [i for i in range(505,645)]\n",
    "\n",
    "results_pages_p1 = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "#add all the urls to nlist\n",
    "\n",
    "#uncomment nlist to intialize it, but recomment so you don't write over it\n",
    "# nlist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7103\n"
     ]
    }
   ],
   "source": [
    "for results_page in results_pages_p1:\n",
    "    sauce1 = urllib.request.urlopen(f'https://www.dfps.state.tx.us/Child_Care/Search_Texas_Child_Care/ppFacilitySearchResults.asp?res_Care_Flag=F&txt_Operation_Name=&txt_Street_Address=&txt_Location_City=&slt_County=&txt_Zip_Code=&slt_Operation_Type=&slt_Issuance_Type=&btn_facDCSearch=Search&chk_Infant=&chk_Toddler=&chk_PreK=&chk_School=&chk_Subsidized=&chk_Special_Needs=&chk_Get_Well=&chk_Special_Skills=&chk_Dropin_Care=&chk_Field_Trips=&chk_Part_Week_Care=&chk_After_School_Care=&chk_Accredited=&chk_Transportation=&chk_Weekend_Care=&chk_Schl_Age_Care=&chk_Bfr_Schl_Care=&chk_Meals_Prvdd=&chk_Snks_Prvdd=&chk_Nght_Care=&pagenum={results_page}').read()\n",
    "    soup1 = bs.BeautifulSoup(sauce1)\n",
    "    #find table and save then find table rows\n",
    "    table2 = soup1.table\n",
    "    table_rows2 = table2.find_all('tr')\n",
    "\n",
    "    #create a list of all rows from table\n",
    "    list = []\n",
    "    for o in table_rows2:\n",
    "        x = o.find_all('a')\n",
    "        #grab href from each 'a' tag in any rows & add it to your list\n",
    "        for m in x:\n",
    "            list.append(m.get('href'))\n",
    "\n",
    "    #narrow your list to only facitlity links\n",
    "    list = [o for o in list if 'ppFacilityDetails' in str(o)]\n",
    "\n",
    "\n",
    "    [nlist.append(n) for n in list if n not in nlist] \n",
    "    print(len(nlist))\n",
    "    pages = nlist\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "url_df2 = pd.DataFrame(pages, columns=['search_page_urls'])\n",
    "url_df2.to_csv('./search_page_urls2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "## create a list urls to get the info from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "## scrape what you need of each page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "# this works \"mostly\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "# Trying to Multiprocess here Version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# start here but let it save so recomment this\n",
    "# page_infos = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3104\n"
     ]
    }
   ],
   "source": [
    "print(len(page_infos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7103\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a few stop points\n",
    "\n",
    "# spu = pd.read_csv('search_page_urls.csv')\n",
    "spu2 = pd.read_csv('search_page_urls2.csv')\n",
    "\n",
    "max2 = len(spu2.index)\n",
    "print(max2)\n",
    "# run the first 20 and time it\n",
    "\n",
    "\n",
    "# count_a1 = [i for i in range(0,10)] #5 min @ 2/minute?\n",
    "count_a1 = [i for i in range(0,900)] #15min?\n",
    "count_a1 = [i for i in range(901,2000)] #18min?\n",
    "count_a1 = [i for i in range(2000,4000)] #20min?\n",
    "count_a1 = [i for i in range(4001,5000)] #20min?\n",
    "count_a1 = [i for i in range(5000,7100)] #20min?\n",
    "count_a1 = [i for i in range(7100,7103)] #20min?\n",
    "# count_a1 = [i for i in range(4001,5000)] #20min?\n",
    "\n",
    "\n",
    "\n",
    "# count_a1 = [i for i in range(5000, 5780)]\n",
    "\n",
    "pages = spu2.loc[count_a1,['search_page_urls']]\n",
    "\n",
    "next_pages = pages['search_page_urls']\n",
    "len(next_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "#### function using regex & beautiful soup to scrape single pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "#full 2  :\n",
    "def scrape_single_page(page):\n",
    "#     for page in next_pages:\n",
    "    link = 'NaN'\n",
    "    op = 'NaN'\n",
    "    hours = 'NaN' \n",
    "    days = 'NaN'\n",
    "    eal = []\n",
    "    sauce = urllib.request.urlopen\\\n",
    "    (f'https://www.dfps.state.tx.us/Child_Care/Search_Texas_Child_Care/{page}').read()\n",
    "    soup = bs.BeautifulSoup(sauce)\n",
    "\n",
    "    #find table\n",
    "    table = soup.table\n",
    "    #find tabe rows\n",
    "    table_rows = table.find_all('tr')\n",
    "\n",
    "    rows_list = []\n",
    "    # table_rows = table_rows\n",
    "    for tr in table_rows:\n",
    "        td = tr.find_all('td')\n",
    "        row = [i.text for i in td]\n",
    "        rows_list.append(row)\n",
    "    tz = rows_list[0]\n",
    "    tz = tz[0]\n",
    "\n",
    "    #grab op#,web address, hours, days\n",
    "\n",
    "\n",
    "    for o in re.findall('Operation Number:\\n\\n\\n\\r\\n.*',tz):\n",
    "        full_op = o\n",
    "        for i in re.findall('\\d*\\d', full_op):\n",
    "            op = i\n",
    "    for line in re.findall('www.*.com', tz):\n",
    "        link = line\n",
    "    for h in re.findall( '\\d.*PM', tz):\n",
    "        hours = h\n",
    "    if re.findall('\\w.*day',tz) != 'Monday - Sunday':\n",
    "        for d in re.findall('\\w.*day',tz):\n",
    "            days = d    \n",
    "            hours = days+ str(' ') + hours\n",
    "    else:\n",
    "        days = 'all_days'    \n",
    "    #save email and link as a list:\n",
    "    eal= [op , link, hours, days\n",
    "         ]\n",
    "\n",
    "    return eal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "#### use multiprocessing to run scrape_single... quickly"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "gradient": {}
   },
   "source": [
    "with concurrent.futures.ProcessPoolExecutor() as executer:\n",
    "    page = [page for page in next_pages]\n",
    "    results = executer.map(scrape_single_page, page)\n",
    "    for result in results:\n",
    "        page_infos.append(result)\n",
    "        print(len(page_infos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### print the end of the dataframe you've built so far\n",
    "#### save it as ndf4 (new dataframe 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {},
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(len(page_infos))\n",
    "#turn your page infos list into a dataframe\n",
    "# ndf = pd.DataFrame(page_infos, columns=['operation_#', 'website_url','hours','days'])\n",
    "#export this new df into a csv\n",
    "\n",
    "# ndf.to_csv('./ndf5.csv', index=False)\n",
    "\n",
    "#print the end of the df to look at your amazing work\n",
    "# ndf.iloc[-22:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {}
   },
   "source": [
    "#### concatenate all df's so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>operation_#</th>\n",
       "      <th>website_url</th>\n",
       "      <th>hours</th>\n",
       "      <th>days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16889</th>\n",
       "      <td>64061.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Monday - Friday 07:00 AM-05:13 PM</td>\n",
       "      <td>Monday - Friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16890</th>\n",
       "      <td>882441.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Monday - Friday 07:00 AM-05:30 PM</td>\n",
       "      <td>Monday - Friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16891</th>\n",
       "      <td>823040.0</td>\n",
       "      <td>www.freewebs.com</td>\n",
       "      <td>Monday - Friday 07:00 AM-06:00 PM</td>\n",
       "      <td>Monday - Friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16892</th>\n",
       "      <td>1705060.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Monday - Saturday 08:00 AM-06:00 PM</td>\n",
       "      <td>Monday - Saturday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16893</th>\n",
       "      <td>547079.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Monday - Friday 09:00 AM-07:00 PM</td>\n",
       "      <td>Monday - Friday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       operation_#       website_url                                hours  \\\n",
       "16889      64061.0               NaN    Monday - Friday 07:00 AM-05:13 PM   \n",
       "16890     882441.0               NaN    Monday - Friday 07:00 AM-05:30 PM   \n",
       "16891     823040.0  www.freewebs.com    Monday - Friday 07:00 AM-06:00 PM   \n",
       "16892    1705060.0               NaN  Monday - Saturday 08:00 AM-06:00 PM   \n",
       "16893     547079.0               NaN    Monday - Friday 09:00 AM-07:00 PM   \n",
       "\n",
       "                    days  \n",
       "16889    Monday - Friday  \n",
       "16890    Monday - Friday  \n",
       "16891    Monday - Friday  \n",
       "16892  Monday - Saturday  \n",
       "16893    Monday - Friday  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import all ndf.csvs to a list\n",
    "csvs = [f for f in os.listdir('.') if f.startswith('ndf')]\n",
    "\n",
    "# remeber you had to use axis=0 and specifically ignore_index=True\n",
    "website_hours_scraped = pd.concat((pd.read_csv(f) for f in csvs), axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "website_hours_scraped.to_csv('./TX_website_and_hours_scraped.csv', index=False)\n",
    "website_hours_scraped.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>operation_#</th>\n",
       "      <th>website_url</th>\n",
       "      <th>hours</th>\n",
       "      <th>days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12867</th>\n",
       "      <td>1526491.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Monday - Friday 06:30 AM-06:00 PM</td>\n",
       "      <td>Monday - Friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12868</th>\n",
       "      <td>1715417.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Monday - Friday 07:15 AM-05:45 PM</td>\n",
       "      <td>Monday - Friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12869</th>\n",
       "      <td>866546.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Monday - Saturday 07:00 AM-06:00 PM</td>\n",
       "      <td>Monday - Saturday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12870</th>\n",
       "      <td>543781.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Monday - Sunday 05:00 AM-07:30 PM</td>\n",
       "      <td>Monday - Sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12871</th>\n",
       "      <td>997866.0</td>\n",
       "      <td>www.learningandlove.com</td>\n",
       "      <td>Monday - Friday 07:15 AM-06:00 PM</td>\n",
       "      <td>Monday - Friday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       operation_#              website_url  \\\n",
       "12867    1526491.0                      NaN   \n",
       "12868    1715417.0                      NaN   \n",
       "12869     866546.0                      NaN   \n",
       "12870     543781.0                      NaN   \n",
       "12871     997866.0  www.learningandlove.com   \n",
       "\n",
       "                                     hours               days  \n",
       "12867    Monday - Friday 06:30 AM-06:00 PM    Monday - Friday  \n",
       "12868    Monday - Friday 07:15 AM-05:45 PM    Monday - Friday  \n",
       "12869  Monday - Saturday 07:00 AM-06:00 PM  Monday - Saturday  \n",
       "12870    Monday - Sunday 05:00 AM-07:30 PM    Monday - Sunday  \n",
       "12871    Monday - Friday 07:15 AM-06:00 PM    Monday - Friday  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_hours_scraped = website_hours_scraped\n",
    "website_hours_scraped.drop_duplicates(subset=['operation_#'],inplace=True)\n",
    "website_hours_scraped.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
